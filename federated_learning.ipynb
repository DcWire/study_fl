{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7744187e-9a74-440b-95b8-557b723fc23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets\n",
    "import fastai \n",
    "from torchvision.transforms import ToTensor\n",
    "# from fastai.data.core import DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from fastai.data.core import DataLoaders\n",
    "from fastai.callback.core import Callback\n",
    "from fastai.vision.all import Learner, Metric\n",
    "from fastai import optimizer\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be5849ab-a4ca-4515-97b5-35c90615be8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3c4d276-b524-46c7-b31a-8324a4b2b124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([256, 1, 28, 28])\n",
      "Shape of y: torch.Size([256]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b180ad30-e4fe-4af6-9367-d3e5b03bc63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0af6c2a9-8714-4c3f-bf16-44b3b964ac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clients = 5\n",
    "train_size = len(training_data)\n",
    "# indices = list(range(train_size))\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.random.manual_seed(RANDOM_SEED)\n",
    "indices = torch.randperm(train_size).tolist()\n",
    "\n",
    "subset_size = train_size // num_clients\n",
    "client_subsets = [] \n",
    "for i in range(num_clients):\n",
    "    start_idx = i * subset_size\n",
    "    end_idx = start_idx + subset_size\n",
    "\n",
    "    if i == num_clients - 1:\n",
    "        end_idx = train_size\n",
    "\n",
    "    subset_indices = indices[start_idx:end_idx]\n",
    "    client_subsets.append(Subset(training_data, subset_indices))\n",
    "\n",
    "client_loaders = [DataLoader(sub, batch_size=batch_size, shuffle=True) for sub in client_subsets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "831d6af4-3ad3-4d67-8681-472ba8624385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits \n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "original_shapes = []\n",
    "for p in model.parameters():\n",
    "    original_shapes.append(p.shape)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ab18bf1-90d6-4bde-a5fc-3ebb2766e207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-1.4421e-05, -1.4421e-05, -1.4421e-05,  ..., -1.4421e-05,\n",
       "          -1.4421e-05, -1.4421e-05],\n",
       "         [-1.4421e-05, -1.4421e-05, -1.4421e-05,  ..., -1.4421e-05,\n",
       "          -1.4421e-05, -1.4421e-05],\n",
       "         [-1.4421e-05, -1.4421e-05, -1.4421e-05,  ..., -1.4421e-05,\n",
       "          -1.4421e-05, -1.4421e-05],\n",
       "         ...,\n",
       "         [-1.4421e-05, -1.4421e-05, -1.4421e-05,  ..., -1.4421e-05,\n",
       "          -1.4421e-05, -1.4421e-05],\n",
       "         [-1.4421e-05, -1.4421e-05, -1.4421e-05,  ..., -1.4421e-05,\n",
       "          -1.4421e-05, -1.4421e-05],\n",
       "         [-1.4421e-05, -1.4421e-05, -1.4421e-05,  ..., -1.4421e-05,\n",
       "          -1.4421e-05, -1.4421e-05]], device='cuda:0'),\n",
       " tensor([ 9.9456e-04,  9.9456e-04, -1.0621e+00,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04, -5.5047e-01,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  1.2893e+00,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04, -1.4893e+00,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04, -1.0977e+00,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  2.2819e-01,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  1.3128e+00,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  1.5536e-01,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04, -3.7272e-01,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  2.4838e-01,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,  9.9456e-04,\n",
       "          9.9456e-04,  9.9456e-04], device='cuda:0'),\n",
       " tensor([[-1.2502e-05, -1.2502e-05, -1.2502e-05,  ..., -1.2502e-05,\n",
       "          -1.2502e-05, -1.2502e-05],\n",
       "         [-1.2502e-05, -1.2502e-05, -1.2502e-05,  ..., -1.2502e-05,\n",
       "          -1.2502e-05, -1.2502e-05],\n",
       "         [-1.2502e-05, -1.2502e-05, -1.2502e-05,  ..., -1.2502e-05,\n",
       "          -1.2502e-05, -1.2502e-05],\n",
       "         ...,\n",
       "         [-1.2502e-05, -1.2502e-05, -1.2502e-05,  ..., -1.2502e-05,\n",
       "          -1.2502e-05, -1.2502e-05],\n",
       "         [-1.2502e-05, -1.2502e-05, -1.2502e-05,  ..., -1.2502e-05,\n",
       "          -1.2502e-05, -1.2502e-05],\n",
       "         [-1.2502e-05, -1.2502e-05, -1.2502e-05,  ..., -1.2502e-05,\n",
       "          -1.2502e-05, -1.2502e-05]], device='cuda:0'),\n",
       " tensor([ 5.2780e-04,  5.2780e-04,  1.6856e-01,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04, -2.5222e-01,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04, -6.5138e-01,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04, -7.1164e-01,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  1.8851e+00,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  8.7198e-01,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04, -2.2864e+00,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04, -7.0851e-02,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  1.2645e+00,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  2.5257e-01,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,  5.2780e-04,\n",
       "          5.2780e-04,  5.2780e-04], device='cuda:0'),\n",
       " tensor([[-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "         [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "         [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "         ...,\n",
       "         [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "         [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "         [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003]],\n",
       "        device='cuda:0'),\n",
       " tensor([-0.0067,  0.0012,  0.0070,  0.0123,  0.0054, -0.0365, -0.0411, -0.0032,\n",
       "          0.0244,  0.0194], device='cuda:0')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We now have 5 different datasets, each with some sort of representation of the data that is unknown, ie, we have no \n",
    "# statistical information on the data that each of these clients would have\n",
    "# We now need to implement variations of the 3 protocols, namely, the encoding protocol, the communication protocol and the decoding protocol\n",
    "\n",
    "# For communication protocol for fixed size encoder, we set the seed. So the seed is communicated with the values. \n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Encoders\n",
    "def variable_size_encoder(grad_vectors, mu, p=0.1):\n",
    "    # Lets take p = 0.1\n",
    "    new_grad_vectors = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(grad_vectors)):\n",
    "            mask = torch.rand_like(grad_vectors[i], device=grad_vectors[i].device) < p\n",
    "            Y = torch.empty_like(grad_vectors[i], device=grad_vectors[i].device)\n",
    "            Y[mask] = (grad_vectors[i][mask] - mu[i] * (1-p))/p\n",
    "            Y[~mask] = mu[i]\n",
    "            new_grad_vectors.append(Y)\n",
    "    return new_grad_vectors\n",
    "\n",
    "def fixed_size_encoder(grad_vectors, mu, k=10):\n",
    "    # k can vary\n",
    "    new_grad_vectors = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(grad_vectors)):\n",
    "            shape = grad_vectors[i].shape\n",
    "            # Flattening the parameters to permutate over them\n",
    "            flat_grad = grad_vectors[i].view(-1)\n",
    "            C = shape[-1]\n",
    "            # Get the length of the flat_grad array\n",
    "            d = flat_grad.numel()\n",
    "            # Shuffle the list [1, 2, ... d] and get the first k elements\n",
    "            indices = torch.randperm(C, device=flat_grad.device)[:k]\n",
    "             \n",
    "            mask = torch.zeros(d, dtype=torch.bool, device=flat_grad.device)\n",
    "            mask[indices] = True\n",
    "            \n",
    "            Y = torch.empty_like(flat_grad)\n",
    "            # Encode the parameters\n",
    "            chosen_vals = (d/k)*flat_grad[mask] - ((d-k)/k)*mu[i]\n",
    "            Y[mask] = chosen_vals\n",
    "            Y[~mask] = mu[i]\n",
    "            Y = Y.view(shape)\n",
    "            new_grad_vectors.append(Y)\n",
    "    return new_grad_vectors\n",
    "            \n",
    "            \n",
    "# Decoders \n",
    "def averaging_decoder(grad_vectors_list):\n",
    "    if isinstance(grad_vectors_list, list):\n",
    "        grad_vectors_list = torch.stack(grad_vectors_list, dim=0)\n",
    "    return torch.mean(grad_vectors_list, dim=0)\n",
    "\n",
    "# Communication protocols\n",
    "def sparse_for_variable_size_encoder(encoded_vectors, mu):\n",
    "    final_vectors = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(encoded_vectors)):\n",
    "            flat_vector = encoded_vectors[i].view(-1)\n",
    "            mask = flat_vector != mu[i]\n",
    "            # vals = encoded_vectors[i][mask]\n",
    "            indices = torch.nonzero(mask, as_tuple=False).view(-1)\n",
    "            values = flat_vector[mask]\n",
    "            final_vectors.append(list(zip(indices, values)))\n",
    "\n",
    "    \n",
    "    return final_vectors, mu\n",
    "    \n",
    "def sparse_for_fixed_size_encoder(encoded_vectors, mu):\n",
    "    final_vectors = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(encoded_vectors)):\n",
    "            flat_vector = encoded_vectors[i].view(-1)\n",
    "            mask = torch.zeros(len(flat_vector), dtype=torch.bool, device=flat_vector.device)\n",
    "            mask[flat_vector != mu[i]] = True\n",
    "            values = flat_vector[mask]\n",
    "            final_vectors.append(values)\n",
    "\n",
    "    return final_vectors, mu, SEED\n",
    "\n",
    "def rebuild_from_protocol_1(final_vectors, mu, original_shapes):\n",
    "    rebuilt_vectors = []\n",
    "    with torch.no_grad():\n",
    "        for i, vec_data in enumerate(final_vectors):\n",
    "            num_elements = 1\n",
    "            \n",
    "            for dim_size in original_shapes[i]:\n",
    "                num_elements *= dim_size\n",
    "    \n",
    "            \n",
    "            Y_flat = torch.full((num_elements,), mu[i], dtype=torch.float32, device=mu[i].device)\n",
    "    \n",
    "            indices = torch.tensor([pair[0] for pair in vec_data], dtype=torch.long, device=Y_flat.device)\n",
    "            values = torch.tensor([pair[1] for pair in vec_data], dtype=Y_flat.dtype, device=Y_flat.device)\n",
    "            Y_flat[indices] = values\n",
    "            Y = Y_flat.view(original_shapes[i])\n",
    "            rebuilt_vectors.append(Y)\n",
    "    return rebuilt_vectors\n",
    "\n",
    "def rebuild_from_protocol_2(final_vectors, mu, SEED, original_shapes):\n",
    "    rebuilt_vectors = []\n",
    "    with torch.no_grad():\n",
    "        for i, values in enumerate(final_vectors):\n",
    "            \n",
    "            num_elements = 1\n",
    "            for dim_size in original_shapes[i]:\n",
    "                num_elements *= dim_size\n",
    "    \n",
    "\n",
    "            torch.manual_seed(SEED) \n",
    "            k = len(final_vectors[i])\n",
    "            indices = torch.randperm(num_elements)[:k]\n",
    "    \n",
    "            Y_flat = torch.full((num_elements,), mu[i], dtype=torch.float32, device=mu[i].device)\n",
    "    \n",
    "            # Place the chosen values\n",
    "            Y_flat[indices] = values\n",
    "    \n",
    "            # Reshape to original shape\n",
    "            Y = Y_flat.view(original_shapes[i])\n",
    "            rebuilt_vectors.append(Y)\n",
    "    return rebuilt_vectors\n",
    "\n",
    "parameters = list(model.parameters())\n",
    "mu_1 = []\n",
    "with torch.no_grad():\n",
    "    for p in parameters:\n",
    "        mu_1.append(torch.mean(p))\n",
    "\n",
    "encoded_vectors = fixed_size_encoder(parameters, mu_1)\n",
    "final_vectors, mu, SEED = sparse_for_fixed_size_encoder(encoded_vectors, mu_1)\n",
    "rebuild_from_protocol_2(final_vectors, mu_1, SEED, original_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3c6e93-87af-426a-8ad5-a81bcba62990",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.FloatTensor([[1, 2, 3], [2, 3, 4], [4, 5, 6]])\n",
    "averaging_decoder(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "311d5661-e5fd-4efc-b2d9-f238953a7c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 784])\n",
      "torch.Size([512, 784])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([10, 512])\n",
      "torch.Size([10, 512])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "parameters = list(model.parameters())\n",
    "variable_size_encoder(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "82afcf7a-e4de-49d7-a835-411608e3731a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProxSGDWithLinearSearch:\n",
    "    def __init__(self, params, lr):\n",
    "        self.params, self.lr = list(params), lr\n",
    "        self.state = {p: {} for p in self.params}\n",
    "        self.hypers = [{'lr': lr}]\n",
    "        self.max_iter = 5\n",
    "        self.eta = 1e-5\n",
    "        \n",
    "    def soft_threshold(self, x, eta):\n",
    "        # Apply the soft-thresholding operator\n",
    "        return F.softshrink(x, lambd=eta)\n",
    "        \n",
    "    def prox_operator(self, x):\n",
    "        # Use the soft-thresholding operator as the proximal step\n",
    "        return self.soft_threshold(x, self.eta)\n",
    "\n",
    "    def Gt(self, x, step_size, x_grad):\n",
    "        return (1/step_size) * (x - self.prox_operator(x - step_size * x_grad))\n",
    "        \n",
    "    def step(self, *args, **kwargs):\n",
    "        model = kwargs.get(\"model\")\n",
    "        loss_fn = kwargs.get(\"loss_fn\")\n",
    "        X = kwargs.get(\"X\")\n",
    "        y = kwargs.get(\"y\")\n",
    "        \n",
    "        orig_params = [p.data.clone() for p in self.params]\n",
    "        step_size = self.lr\n",
    "        with torch.no_grad():\n",
    "            pred = model(X)\n",
    "            old_loss = loss_fn(pred, y)\n",
    "        \n",
    "        for _ in range(self.max_iter):\n",
    "            for p in self.params:\n",
    "                if p.grad is not None: \n",
    "                    Gt_val = self.Gt(p.data, step_size, p.grad.data)\n",
    "                    p.data = p.data - step_size * Gt_val\n",
    "            with torch.no_grad():\n",
    "                pred = model(X)\n",
    "                new_loss = loss_fn(pred, y)\n",
    "            if new_loss < old_loss:\n",
    "                break\n",
    "            else:\n",
    "                for i, j in zip(self.params, orig_params):\n",
    "                    i.data.copy_(j)\n",
    "                step_size *= 0.5\n",
    "        else: \n",
    "            for p in self.params:\n",
    "                if p.grad is not None: \n",
    "                    Gt_val = self.Gt(p.data, step_size, p.grad.data)\n",
    "                    p.data = p.data - step_size * Gt_val\n",
    "        self.lr = step_size\n",
    "    def zero_grad(self, *args, **kwargs):\n",
    "        for p in self.params:\n",
    "            p.grad = None\n",
    "\n",
    "    def set_hypers(self, **kwargs):\n",
    "        if 'lr' in kwargs:\n",
    "            self.lr = kwargs['lr']\n",
    "            self.hypers[0]['lr'] = kwargs['lr']\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c008223f-22d0-4b30-bf02-6ad83feccae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step(model=model, loss_fn=loss_fn, X=X, y=y)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f\"BATCH: {batch} of {size/batch_size} batches\")\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "390563a7-8ac8-4947-8a2e-cbda1e90dd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d30967-d2e7-4eca-93ae-225309a9ef1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
