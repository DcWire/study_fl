{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5e54e8c",
   "metadata": {},
   "source": [
    "# Part II\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Federated Learning is a machine learning paradigm aimed at training a high-quality centralized model while keeping the training data distributed across numerous clients. This approach is particularly useful in scenarios where network connections may be unreliable or slow, and it ensures data privacy by allowing models to be trained without the need to access or move the data.\n",
    "\n",
    "Traditionally, AI applications—such as spam filters, chatbots, and recommendation systems—were trained on massive datasets collected in centralized locations. These datasets often came from user contributions in exchange for free services like email or music streaming.\n",
    "\n",
    "However, the landscape of AI is shifting towards a decentralized approach. Instead of aggregating data in a single location, modern AI models are being trained collaboratively at the edge—on devices like smartphones, laptops, or private servers. This data never leaves its original source, safeguarding user privacy.\n",
    "\n",
    "Federated Learning is becoming the standard for addressing growing regulatory demands on data handling and storage. By processing data locally, it also enables efficient use of raw data generated by sensors on satellites, industrial machinery, smart home devices, and wearable technology, all without the need to transfer data to central servers.\n",
    "\n",
    "As described in Part I, the parameter update rule in a distributed setting can be expressed as:\n",
    "\n",
    "$$\n",
    "x_{t+1} = x_t - \\eta \\frac{1}{N} \\sum_{i=1}^N \\nabla f_i(x_t)\n",
    "$$\n",
    "*where:*\n",
    "- $ N $ : Number of devices\n",
    "- $ f_i(x_t) $ : Loss function for data on device $ i $ \n",
    "\n",
    "In this project, I’ve implemented various concepts from the paper [Randomized Distributed Mean Estimation: Accuracy vs. Communication](https://www.frontiersin.org/journals/applied-mathematics-and-statistics/articles/10.3389/fams.2018.00062/full), which provides a foundational understanding of Federated Learning. This paper also introduces methods to reduce the information exchanged between the master and client nodes using specific encoding and communication protocols, optimizing bandwidth usage.\n",
    "\n",
    "### Project Prerequisites\n",
    "\n",
    "Before diving into this section of the project, it’s recommended to review Part I, as the implementation here builds on the *Proximal SGD* algorithm developed earlier. Additionally, a basic understanding of *Linear Algebra* is sufficient to follow the concepts discussed in this section. \n",
    "\n",
    "### Implementation Overview\n",
    "\n",
    "In this project, I’ve utilized the MNIST dataset once again, dividing it into 5 parts for 5 clients. Each client is assigned its own model, which is trained on a subset of the dataset. The setup establishes a Federated Learning framework where:\n",
    "1. A global model is initialized and distributed to the client nodes.\n",
    "2. Each client trains its local model on its respective data subset.\n",
    "3. The locally updated models are sent back to the master node.\n",
    "4. The master node aggregates the updates and refines the global model.\n",
    "\n",
    "This process is repeated iteratively until the global model converges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7744187e-9a74-440b-95b8-557b723fc23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets\n",
    "import fastai \n",
    "from torchvision.transforms import ToTensor\n",
    "# from fastai.data.core import DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from fastai.data.core import DataLoaders\n",
    "from fastai.callback.core import Callback\n",
    "from fastai.vision.all import Learner, Metric\n",
    "from fastai import optimizer\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be5849ab-a4ca-4515-97b5-35c90615be8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3c4d276-b524-46c7-b31a-8324a4b2b124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([256, 1, 28, 28])\n",
      "Shape of y: torch.Size([256]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b180ad30-e4fe-4af6-9367-d3e5b03bc63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0af6c2a9-8714-4c3f-bf16-44b3b964ac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clients = 5\n",
    "train_size = len(training_data)\n",
    "# indices = list(range(train_size))\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.random.manual_seed(RANDOM_SEED)\n",
    "indices = torch.randperm(train_size).tolist()\n",
    "\n",
    "subset_size = train_size // num_clients\n",
    "client_subsets = [] \n",
    "for i in range(num_clients):\n",
    "    start_idx = i * subset_size\n",
    "    end_idx = start_idx + subset_size\n",
    "\n",
    "    if i == num_clients - 1:\n",
    "        end_idx = train_size\n",
    "\n",
    "    subset_indices = indices[start_idx:end_idx]\n",
    "    client_subsets.append(Subset(training_data, subset_indices))\n",
    "\n",
    "client_loaders = [DataLoader(sub, batch_size=batch_size, shuffle=True) for sub in client_subsets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "831d6af4-3ad3-4d67-8681-472ba8624385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits \n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "original_shapes = []\n",
    "for p in model.parameters():\n",
    "    original_shapes.append(p.shape)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d2e38d",
   "metadata": {},
   "source": [
    "### The Three Protocols \n",
    "\n",
    "For the entire process of training the global model, I need 3 protocols, an encoding protocol, communication protocol and decoding protocol. I implemented the variable size encoder, sparse communication protocol for variable size encoder and functions for rebuilding the model weights and then the same for fixed size encoder. I update the global model with the average of the updates from the clients. \n",
    "\n",
    "### Variable Size Encoder\n",
    "\n",
    "For each position (i, j) in the weights, we associate a parameter $ p_{ij}, 0 < p_{ij} <= 1 $. This parameter represents the probability of retaining the actual gradient value. The encoding protocol is defined as: \n",
    "\n",
    "$$\n",
    "Y_i(j) =\n",
    "\\begin{cases}\n",
    "\\frac{X_i(j) - \\mu_i}{p_{ij}(1 - p_{ij})} & \\text{with probability } p_{ij}, \\\\\n",
    "\\mu_i & \\text{with probability } 1 - p_{ij},\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $X_i(j)$ : Original gradient value at position $(i, j)$,\n",
    "- $\\mu_i$ : Predefined constant (e.g., the mean of the gradient vector),\n",
    "- $p_{ij}$ : Probability of encoding the gradient.\n",
    "\n",
    "This probabilistic approach ensures that only a fraction of the data is communicated while retaining the essential information required to reconstruct the original gradient vector.\n",
    "\n",
    "#### Key Properties of the Encoding Protocol\n",
    "\n",
    "\t1.\tUnbiasedness:\n",
    "The encoded gradient vector is an unbiased estimate of the original gradient. That is:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_\\alpha[Y_i(j)] = X_i(j),\n",
    "$$\n",
    "\n",
    "ensuring that the expected value of the encoded vector matches the original vector.\n",
    "\n",
    "\t2.\tMean Squared Error (MSE):\n",
    "The Mean Squared Error (MSE) of the encoded vector is given by:\n",
    "\n",
    "$$\n",
    "\\text{MSE}_{\\alpha}(X_1, \\dots, X_n) = \\frac{1}{n^2} \\sum_{i, j} \\left( \\frac{1 - p_{ij}}{p_{ij}} \\right) (X_i(j) - \\mu_i)^2.\n",
    "$$\n",
    "\n",
    "This equation demonstrates that smaller probabilities  $p_{ij}$  lead to larger reconstruction errors, highlighting a trade-off between compression and accuracy.\n",
    "\n",
    "#### Code Explanation\n",
    "\n",
    "The Variable Size Encoder is implemented in three main parts:\n",
    "\n",
    "1. Encoding Gradients (*variable_size_encoder*):\n",
    "    \n",
    "- This function encodes gradient vectors using the described protocol.\n",
    "- A random mask is generated for each position based on the probability matrix.\n",
    "- Based on the mask, the encoded matrix is computed.\n",
    "        \n",
    "2. Sparse Representation (*sparse_for_variable_size_encoder*):\n",
    "    \n",
    "-  This function compresses the encoded vectors by storing only non-$\\mu$  values along with their indices. This reduces the data size for communication.\n",
    "- The sparse representation is a list of index-value pairs for each vector.\n",
    "        \n",
    "3. Reconstructing Gradients (*rebuild_from_protocol_1*):\n",
    "    \n",
    "- This function reconstructs the original gradient vectors from the sparse representation.\n",
    "- The indices and values are used to restore the original positions, while positions not in the sparse representation are set to  $\\mu$ .\n",
    "\n",
    "### Fixed Size Encoder\n",
    "\n",
    "The *Fixed Size Encoder* is an alternative encoding protocol designed to ensure a deterministic communication cost, irrespective of the data distribution. Unlike the *Variable Size Encoder*, where the number of transmitted values depends on probabilities  $p_{ij}$ , the Fixed Size Encoder guarantees that exactly  k  values are communicated for each gradient vector. This is achieved by randomly selecting  k  positions in the gradient vector for encoding.\n",
    "\n",
    "For a gradient vector  $X_i$  of size  $d$ , the encoding protocol works as follows:\n",
    "1.\tSample a subset  $D_i$  of  k  indices uniformly at random from  $\\{1, 2, \\dots, d\\}$ .\n",
    "2.\tFor each index  $j$ , encode the value  $Y_i(j)$  as:\n",
    "\n",
    "$$\n",
    "Y_i(j) =\n",
    "\\begin{cases}\n",
    "\\frac{d}{k} X_i(j) - \\frac{d-k}{k} \\mu_i & \\text{if } j \\in D_i, \\\\\n",
    "\\mu_i & \\text{otherwise},\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $X_i(j)$ : Original gradient value at position $j$ ,\n",
    "- $\\mu_i$ : A predefined constant (e.g., the mean of the gradient vector),\n",
    "- $d$ : Total number of elements in the vector,\n",
    "- $k$ : Fixed number of encoded elements.\n",
    "\n",
    "This ensures that the size of the support of  $Y_i$  is always  k , i.e.,  $|S_i| = k$ .\n",
    "\n",
    "#### Key Properties of the Encoding Protocol\n",
    "\n",
    "    1.\tUnbiasedness:\n",
    "The encoded gradient vector is an unbiased estimate of the original gradient. That is:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_\\alpha[Y_i(j)] = X_i(j).\n",
    "$$\n",
    "\n",
    "    2.\tMean Squared Error (MSE):\n",
    "The Mean Squared Error (MSE) of the encoded vector is given by:\n",
    "\n",
    "$$\n",
    "\\text{MSE}_{\\alpha}(X_1, \\dots, X_n) = \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^d \\left( \\frac{d-k}{k} \\right) (X_i(j) - \\mu_i)^2.\n",
    "$$\n",
    "\n",
    "#### Code Explanation\n",
    "\n",
    "The Fixed Size Encoder is implemented in three main parts:\n",
    "\n",
    "1. Encoding Gradients (*fixed_size_encoder*)\n",
    "\n",
    "This function encodes gradient vectors using the fixed-size encoding protocol:\n",
    "\n",
    "- A random subset of  k  indices is sampled from the gradient vector.\n",
    "- For the sampled indices, the values are transformed using the encoding formula.\n",
    "- For other indices, the value is set to  $\\mu$ .\n",
    "\n",
    "2. Sparse Representation (*sparse_for_fixed_size_encoder*)\n",
    "\n",
    "This function compresses the encoded vectors by storing only the non-$\\mu$  values. It creates a sparse representation that reduces communication overhead.\n",
    "\n",
    "3. Reconstructing Gradients (*rebuild_from_protocol_2*)\n",
    "\n",
    "This function reconstructs the original gradient vectors from the sparse representation:\n",
    "- The indices of selected elements are regenerated using the same random seed.\n",
    "- The sparse values are placed at the appropriate indices, while other positions are filled with $\\mu$ .\n",
    "\n",
    "### Averaging Decoder\n",
    "\n",
    "For updating the global model, we are going to take mean of the model updates from the clients and update the global model with that mean.\n",
    "\n",
    "Given the model updates  $U_1, U_2, \\dots, U_N$  from  $N$  clients, the global model update  $G$  is computed as:\n",
    "\n",
    "$$\n",
    "G = \\frac{1}{N} \\sum_{i=1}^N U_i,\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $U_i$  represents the update from the  $i$-th client, typically the difference between the client’s updated model parameters and the global model parameters,\n",
    "- $N$  is the total number of participating clients.\n",
    "\n",
    "The global model is then updated as:\n",
    "\n",
    "$$\n",
    "W_{\\text{global}} \\gets W_{\\text{global}} + G,\n",
    "$$\n",
    "\n",
    "where  $W_{\\text{global}}$  represents the parameters of the global model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab18bf1-90d6-4bde-a5fc-3ebb2766e207",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We now have 5 different datasets, each with some sort of representation of the data that is unknown, ie, we have no \n",
    "# statistical information on the data that each of these clients would have\n",
    "# We now need to implement variations of the 3 protocols, namely, the encoding protocol, the communication protocol and the decoding protocol\n",
    "\n",
    "# For communication protocol for fixed size encoder, we set the seed. So the seed is communicated with the values. \n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Encoders\n",
    "def variable_size_encoder(grad_vectors, mu, p=0.1):\n",
    "    # Lets take p = 0.1\n",
    "    new_grad_vectors = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(grad_vectors)):\n",
    "            mask = torch.rand_like(grad_vectors[i], device=grad_vectors[i].device) < p\n",
    "            Y = torch.empty_like(grad_vectors[i], device=grad_vectors[i].device)\n",
    "            Y[mask] = (grad_vectors[i][mask] - mu[i] * (1-p))/p\n",
    "            Y[~mask] = mu[i]\n",
    "            new_grad_vectors.append(Y)\n",
    "    return new_grad_vectors\n",
    "\n",
    "def fixed_size_encoder(grad_vectors, mu, seed, k=40):\n",
    "    # k can vary\n",
    "    orig = k \n",
    "    new_grad_vectors = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(grad_vectors)):\n",
    "            k = orig\n",
    "            shape = grad_vectors[i].shape\n",
    "            # Flattening the parameters to permutate over them\n",
    "            flat_grad = grad_vectors[i].view(-1)\n",
    "            d = flat_grad.numel()\n",
    "            k = min(k, d)\n",
    "            torch.manual_seed(seed)\n",
    "            # Shuffle the list [1, 2, ... d] and get the first k elements\n",
    "            indices = torch.randperm(d, device=flat_grad.device)[:k]\n",
    "            mask = torch.zeros(d, dtype=torch.bool, device=flat_grad.device)\n",
    "            mask[indices] = True\n",
    "            \n",
    "            Y = torch.empty_like(flat_grad)\n",
    "            # Encode the parameters\n",
    "            chosen_vals = (d/k)*flat_grad[mask] - ((d-k)/k)*mu[i]\n",
    "            Y[mask] = flat_grad[mask]\n",
    "            Y[~mask] = mu[i]\n",
    "            Y = Y.view(shape)\n",
    "            new_grad_vectors.append(Y)\n",
    "    return new_grad_vectors\n",
    "            \n",
    "            \n",
    "# Decoders : I wont be making use of this later on\n",
    "def averaging_decoder(grad_vectors_list):\n",
    "    if isinstance(grad_vectors_list, list):\n",
    "        grad_vectors_list = torch.stack(grad_vectors_list, dim=0)\n",
    "    return torch.mean(grad_vectors_list, dim=0)\n",
    "\n",
    "# Communication protocols\n",
    "def sparse_for_variable_size_encoder(encoded_vectors, mu):\n",
    "    final_vectors = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(encoded_vectors)):\n",
    "            flat_vector = encoded_vectors[i].view(-1)\n",
    "            mask = flat_vector != mu[i]\n",
    "            # vals = encoded_vectors[i][mask]\n",
    "            indices = torch.nonzero(mask, as_tuple=False).view(-1)\n",
    "            values = flat_vector[mask]\n",
    "            final_vectors.append(list(zip(indices, values)))\n",
    "\n",
    "    \n",
    "    return final_vectors, mu\n",
    "    \n",
    "def sparse_for_fixed_size_encoder(encoded_vectors, mu, seed):\n",
    "    final_vectors = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(encoded_vectors)):\n",
    "            flat_vector = encoded_vectors[i].view(-1)\n",
    "            mask = torch.zeros(len(flat_vector), dtype=torch.bool, device=flat_vector.device)\n",
    "            mask[flat_vector != mu[i]] = True\n",
    "            values = flat_vector[mask]\n",
    "            final_vectors.append(values)\n",
    "    \n",
    "    return final_vectors, mu, seed\n",
    "\n",
    "def rebuild_from_protocol_1(final_vectors, mu, original_shapes):\n",
    "    rebuilt_vectors = []\n",
    "    with torch.no_grad():\n",
    "        for i, vec_data in enumerate(final_vectors):\n",
    "            num_elements = 1\n",
    "            \n",
    "            for dim_size in original_shapes[i]:\n",
    "                num_elements *= dim_size\n",
    "    \n",
    "            \n",
    "            Y_flat = torch.full((num_elements,), mu[i], dtype=torch.float32, device=mu[i].device)\n",
    "    \n",
    "            indices = torch.tensor([pair[0] for pair in vec_data], dtype=torch.long, device=Y_flat.device)\n",
    "            values = torch.tensor([pair[1] for pair in vec_data], dtype=Y_flat.dtype, device=Y_flat.device)\n",
    "            Y_flat[indices] = values\n",
    "            Y = Y_flat.view(original_shapes[i])\n",
    "            rebuilt_vectors.append(Y)\n",
    "    return rebuilt_vectors\n",
    "\n",
    "def rebuild_from_protocol_2(final_vectors, mu, seed, original_shapes):\n",
    "    rebuilt_vectors = []\n",
    "    with torch.no_grad():\n",
    "        for i, values in enumerate(final_vectors):\n",
    "            \n",
    "            num_elements = 1\n",
    "            for dim_size in original_shapes[i]:\n",
    "                num_elements *= dim_size\n",
    "    \n",
    "            \n",
    "            k = len(values)  # number of chosen elements\n",
    "            d = num_elements\n",
    "            torch.manual_seed(seed)\n",
    "            indices = torch.randperm(d, device=device)[:k]\n",
    "            Y_flat = torch.full((num_elements,), float(mu[i]), dtype=torch.float32, device=mu[i].device)\n",
    "            sorted_indices, sorted_pos = torch.sort(indices)\n",
    "            sorted_values = values[sorted_pos]\n",
    "            Y_flat[sorted_indices] = values\n",
    "            chosen_mask = (Y_flat != mu[i])\n",
    "            rebuilt_vectors.append(Y_flat.view(original_shapes[i]))\n",
    "\n",
    "    return rebuilt_vectors\n",
    "\n",
    "\n",
    "parameters = list(model.parameters())\n",
    "mu_1 = []\n",
    "with torch.no_grad():\n",
    "    for p in parameters:\n",
    "        mu_1.append(torch.mean(p))\n",
    "# print(mu_1)\n",
    "encoded_vectors = fixed_size_encoder(parameters, mu_1, SEED, k=1000)\n",
    "final_vectors, mu, SEED = sparse_for_fixed_size_encoder(encoded_vectors, mu_1, SEED)\n",
    "rebuilt_vectors = rebuild_from_protocol_2(final_vectors, mu_1, SEED, original_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82afcf7a-e4de-49d7-a835-411608e3731a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProxSGDWithLinearSearch:\n",
    "    def __init__(self, params, lr):\n",
    "        self.params, self.lr = list(params), lr\n",
    "        self.state = {p: {} for p in self.params}\n",
    "        self.hypers = [{'lr': lr}]\n",
    "        self.max_iter = 5\n",
    "        self.eta = 1e-5\n",
    "        \n",
    "    def soft_threshold(self, x, eta):\n",
    "        # Apply the soft-thresholding operator\n",
    "        return F.softshrink(x, lambd=eta)\n",
    "        \n",
    "    def prox_operator(self, x):\n",
    "        # Use the soft-thresholding operator as the proximal step\n",
    "        return self.soft_threshold(x, self.eta)\n",
    "\n",
    "    def Gt(self, x, step_size, x_grad):\n",
    "        return (1/step_size) * (x - self.prox_operator(x - step_size * x_grad))\n",
    "        \n",
    "    def step(self, *args, **kwargs):\n",
    "        model = kwargs.get(\"model\")\n",
    "        loss_fn = kwargs.get(\"loss_fn\")\n",
    "        X = kwargs.get(\"X\")\n",
    "        y = kwargs.get(\"y\")\n",
    "        \n",
    "        orig_params = [p.data.clone() for p in self.params]\n",
    "        step_size = self.lr\n",
    "        with torch.no_grad():\n",
    "            pred = model(X)\n",
    "            old_loss = loss_fn(pred, y)\n",
    "        flag = True\n",
    "        for _ in range(self.max_iter):\n",
    "            for p in self.params:\n",
    "                if p.grad is not None: \n",
    "                    Gt_val = self.Gt(p.data, step_size, p.grad.data)\n",
    "                    p.data = p.data - step_size * Gt_val\n",
    "            with torch.no_grad():\n",
    "                pred = model(X)\n",
    "                new_loss = loss_fn(pred, y)\n",
    "            if new_loss < old_loss:\n",
    "                flag = False\n",
    "                break\n",
    "            else:\n",
    "                for i, j in zip(self.params, orig_params):\n",
    "                    i.data.copy_(j)\n",
    "                step_size *= 0.5\n",
    "        if flag: \n",
    "            for p in self.params:\n",
    "                if p.grad is not None: \n",
    "                    Gt_val = self.Gt(p.data, step_size, p.grad.data)\n",
    "                    p.data = p.data - step_size * Gt_val\n",
    "        else:\n",
    "            self.lr = step_size\n",
    "        # print(self.lr)\n",
    "    def zero_grad(self, *args, **kwargs):\n",
    "        for p in self.params:\n",
    "            p.grad = None\n",
    "\n",
    "    def set_hypers(self, **kwargs):\n",
    "        if 'lr' in kwargs:\n",
    "            self.lr = kwargs['lr']\n",
    "            self.hypers[0]['lr'] = kwargs['lr']\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d7f3833-9cb2-459d-86b1-2b11ea031b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c008223f-22d0-4b30-bf02-6ad83feccae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step(model=model, loss_fn=loss_fn, X=X, y=y)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f\"BATCH: {batch} of {size/batch_size} batches\")\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "390563a7-8ac8-4947-8a2e-cbda1e90dd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn, number=\"main\"):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error for client {number}: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298bcdda",
   "metadata": {},
   "source": [
    "#### Code Explanation \n",
    "\n",
    "I developed two classes to simplify debugging and ensure reproducibility of results. The *Client* class includes methods for training and testing the model at the client, setting model parameters, and retrieving encoded parameters. The *Master* class contains methods to update parameters based on the encoding protocol used, along with a method for testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "89d30967-d2e7-4eca-93ae-225309a9ef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Client:\n",
    "    def __init__(self, model, train_dataloader, loss_fn, mu, max_iter=2):\n",
    "        self.model = model\n",
    "        # self.optimizer = ProxSGDWithLinearSearch(model.parameters(), 20)\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.max_iter = max_iter\n",
    "        self.loss_fn = loss_fn\n",
    "        self.mu = mu\n",
    "        \n",
    "    def train(self):\n",
    "        for i in range(0, self.max_iter):\n",
    "            train(self.train_dataloader, self.model, self.loss_fn, ProxSGDWithLinearSearch(self.model.parameters(), 20))\n",
    "\n",
    "    def test(self, test_dataloader, number):\n",
    "        test(test_dataloader, self.model, self.loss_fn, number)\n",
    "\n",
    "    def set_parameters(self, model):\n",
    "        self.model = copy.deepcopy(model)\n",
    "            \n",
    "    def get_encoded_1(self, p):\n",
    "        encoded = variable_size_encoder(list(self.model.parameters()), self.mu, p)\n",
    "        final_vectors, mu = sparse_for_variable_size_encoder(encoded, self.mu)\n",
    "        return final_vectors, mu\n",
    "\n",
    "    def get_encoded_2(self, k, seed):\n",
    "        encoded = fixed_size_encoder(list(self.model.parameters()), self.mu, seed, k)\n",
    "        self.encoded = encoded\n",
    "        final_vectors, mu, seed = sparse_for_fixed_size_encoder(encoded, self.mu, seed)\n",
    "        # encoded_vectors = encoded\n",
    "        return final_vectors, mu, seed\n",
    "\n",
    "    # Built the member function to test if continuous update of the mean is helpful or not\n",
    "    def update_mean(self):\n",
    "        with torch.no_grad():\n",
    "            params = list(self.model.parameters())\n",
    "            self.mu = [torch.mean(p) for p in params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "7eee3638-48bb-4b5a-88ae-40f57b6f97eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Master:\n",
    "    def __init__(self, model, mu, loss_fn):\n",
    "        self.model = model\n",
    "        self.original_shapes = [p.shape for p in model.parameters()]\n",
    "        self.mu = mu\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def set_mean(self, mu):\n",
    "        self.mu = copy.deepcopy(mu)\n",
    "        \n",
    "    def update_global_model_from_protocol_1(self, clients_data):\n",
    "        # clients_data is a list of tuples (final_vectors, mu, p) from each client\n",
    "        # Decode each client's parameters and then compute updates\n",
    "        decoded_params_list = []\n",
    "        for (final_vectors, mu) in clients_data:\n",
    "            decoded = rebuild_from_protocol_1(final_vectors, mu, self.original_shapes)\n",
    "            decoded_params_list.append(decoded)\n",
    "\n",
    "        master_params = list(self.model.parameters())\n",
    "        all_updates = []\n",
    "        with torch.no_grad():\n",
    "            for decoded_params in decoded_params_list:\n",
    "                updates = [(dp - mp) for dp, mp in zip(decoded_params, master_params)]\n",
    "                all_updates.append(updates)\n",
    "\n",
    "            # Average updates across clients\n",
    "            # Stack each parameter across clients and mean\n",
    "            averaged_updates = []\n",
    "            num_clients = len(all_updates)\n",
    "            for param_idx in range(len(master_params)):\n",
    "                stack = torch.stack([all_updates[c][param_idx] for c in range(num_clients)], dim=0)\n",
    "                avg = torch.mean(stack, dim=0)\n",
    "                averaged_updates.append(avg)\n",
    "\n",
    "            # Apply averaged updates to master model\n",
    "            for mp, au in zip(master_params, averaged_updates):\n",
    "                mp.data.add_(au)\n",
    "\n",
    "    def update_global_model_from_protocol_2(self, clients_data):\n",
    "        # clients_data is a list of tuples (final_vectors, mu, seed, k) from each client\n",
    "        decoded_params_list = []\n",
    "        for (final_vectors, mu, seed) in clients_data:\n",
    "            decoded = rebuild_from_protocol_2(final_vectors, mu, seed, self.original_shapes)\n",
    "            decoded_params_list.append(decoded)\n",
    "        self.decoded_params = decoded_params_list\n",
    "        self.clients_data = clients_data\n",
    "        master_params = list(self.model.parameters())\n",
    "        all_updates = []\n",
    "        with torch.no_grad():\n",
    "            for decoded_params in decoded_params_list:\n",
    "                updates = [(dp - mp) for dp, mp in zip(decoded_params, master_params)]\n",
    "                all_updates.append(updates)\n",
    "\n",
    "            num_clients = len(all_updates)\n",
    "            averaged_updates = []\n",
    "            for param_idx in range(len(master_params)):\n",
    "                stack = torch.stack([all_updates[c][param_idx] for c in range(num_clients)], dim=0)\n",
    "                avg = torch.mean(stack, dim=0)\n",
    "                averaged_updates.append(avg)\n",
    "\n",
    "            # Apply averaged updates to master model\n",
    "            \n",
    "            for mp, au in zip(master_params, averaged_updates):\n",
    "                mp.data.add_(au)\n",
    "\n",
    "    def test(self, test_dataloader):\n",
    "        test(test_dataloader, self.model, self.loss_fn, \"master\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "8dedf521-0485-4440-bd22-d9865b54eb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "parameters = list(model.parameters())\n",
    "mu_1 = []\n",
    "with torch.no_grad():\n",
    "    for p in parameters:\n",
    "        mu_1.append(torch.mean(p))\n",
    "# mu_1 = torch.zeros(len(parameters), device=device)        \n",
    "master = Master(model, mu_1, loss_fn) \n",
    "clients = [Client(NeuralNetwork().to(device), client_loaders[i], loss_fn, mu_1, 5) for i in range(num_clients)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2ac3d8-255a-4ef3-9774-c05cfcabb76d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Encoder 1\n",
    "\n",
    "for i in range(1):\n",
    "    # Master sends global parameters to the client\n",
    "    for client in clients:\n",
    "        client.set_parameters(master.model)\n",
    "        # client.update_mean()\n",
    "    \n",
    "    # Clients are trained locally\n",
    "    for client in clients:\n",
    "        client.train()\n",
    "    \n",
    "    clients_data_protocol_1 = []\n",
    "    for client in clients:\n",
    "        final_vectors, mu = client.get_encoded_1(p=0.5)\n",
    "        clients_data_protocol_1.append((final_vectors, mu))\n",
    "    \n",
    "    master.update_global_model_from_protocol_1(clients_data_protocol_1)\n",
    "\n",
    "    master.test(test_dataloader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f16af22-87fe-4c1b-aec4-13d90897e979",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Encoder 2\n",
    "base_seed = 42\n",
    "\n",
    "for i in range(5):\n",
    "    # Master sends global parameters to the client\n",
    "    for client in clients:\n",
    "        client.set_parameters(master.model)\n",
    "        client.update_mean()\n",
    "    \n",
    "    # Clients are trained locally\n",
    "    for client in clients:\n",
    "        client.train()\n",
    "    \n",
    "    clients_data_protocol_2 = []\n",
    "    for client_id, client in enumerate(clients):\n",
    "        current_seed = base_seed + i * 1000 + client_id\n",
    "        torch.manual_seed(current_seed)\n",
    "        final_vectors, mu, seed = client.get_encoded_2(k=5000000, seed=current_seed)\n",
    "        clients_data_protocol_2.append((final_vectors, mu, seed))\n",
    "    \n",
    "    master.update_global_model_from_protocol_2(clients_data_protocol_2)\n",
    "    \n",
    "    master.test(test_dataloader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "f90500bc-dd79-4fca-b658-631c250078d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error for client master: \n",
      " Accuracy: 97.8%, Avg loss: 0.068344 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "master.test(test_dataloader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e6f229f0-ecaf-4274-bf1d-0ae481f367c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error for client 0: \n",
      " Accuracy: 93.8%, Avg loss: 0.199948 \n",
      "\n",
      "Test Error for client 1: \n",
      " Accuracy: 94.3%, Avg loss: 0.184125 \n",
      "\n",
      "Test Error for client 2: \n",
      " Accuracy: 94.2%, Avg loss: 0.184488 \n",
      "\n",
      "Test Error for client 3: \n",
      " Accuracy: 94.6%, Avg loss: 0.170945 \n",
      "\n",
      "Test Error for client 4: \n",
      " Accuracy: 94.0%, Avg loss: 0.196233 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(clients)):\n",
    "    clients[i].test(test_dataloader, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801eec2e-9a80-40c7-9914-421724b96b41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
