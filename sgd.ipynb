{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5671b66f",
   "metadata": {},
   "source": [
    "# Part I\n",
    "\n",
    "## Introduction\n",
    "*Gradient descent* is one of the most popular and widely used optimization algorithms in machine learning. It serves as the backbone for many other optimization algorithms, such as *stochastic gradient descent (SGD)*. This project aims to study *gradient descent* and its variants to understand their role in optimization algorithms.\n",
    "The problem we would like to optimize can be formulated as:\n",
    "\n",
    "$$\n",
    "\\min_{x \\in \\mathbb{R}^d} \\ f(x) + R(x),\n",
    "$$\n",
    "*where:*\n",
    "- $f(x)$  is a convex, smooth, and differentiable function, representing the cost function (the primary objective).\n",
    "- $R(x)$  is a proximable regularization term, which may not necessarily be differentiable.\n",
    "\n",
    "\n",
    "So given the problem, the updates to the model weights (x) can be formulated using *gradient descent* as: \n",
    "\n",
    "$$\n",
    "x_{t+1} = x_t - \\eta \\nabla f(x_t)\n",
    "$$\n",
    "*where:*\n",
    "- $ x_t $ : Current parameters\n",
    "- $ \\eta $ : Learning rate\n",
    "- $ \\nabla f(x_t) $ : Gradient of the loss function with respect to parameters\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "In many real-world scenarios, machine learning models must work with large datasets, making full-batch computations computationally inconceivable. *Stochastic gradient descent (SGD)* plays a crucial role in addressing this challenge. Instead of processing the entire dataset at once, *SGD* uses a randomly selected subset of data points, significantly reducing the computational burden.\n",
    "The objective function in such cases can be represented as:\n",
    "\n",
    "$$\n",
    "f(x) = \\mathbb{E}_{\\xi \\sim \\mathcal{D}} \\left[ f_\\xi(x) \\right],\n",
    "$$\n",
    "*where:*\n",
    "- $ \\xi $  is a random variable sampled from the data distribution $ \\mathcal{D} $,\n",
    "- $ f_\\xi $ : $ \\mathbb{R}^d \\to \\mathbb{R} $  is a smooth function for each  $ \\xi $.\n",
    "- $ \\mathbb{E}_{\\xi \\sim \\mathcal{D}} \\left[ f_\\xi(x) \\right] $ : We expect the objective function calculated using a subset of the distribution same as the one calculated using the entire dataset.\n",
    "\n",
    "Several variants of stochastic gradient descent have been proposed to address specific challenges. For instance:\n",
    "- Proximal SGD is designed to handle non-differentiable regularization terms.\n",
    "- Minibatch SGD uses batches of data and trains the model.\n",
    "- SVRG (Stochastic Variance Reduced Gradient) aims to reduce the variance of gradient estimates.\n",
    "\n",
    "In this project, I have incorporated the L1 penalty (which is non-differentiable) and implemented proximal SGD alongside basic SGD. A detailed discussion of these algorithms will follow in later sections. \n",
    "\n",
    "### Role of Stochastic Gradient Descent in Distributed Training\n",
    "The idea of training from a subset of data points is extended to distribute training across multiple devices. *Federated learning* is one such class of algorithms that leverages this concept. In *federated learning*, a model is trained collaboratively across several machines, where the data is partitioned among these devices.\n",
    "\n",
    "A *master* device maintains the global model, while *client* devices receive the global model, train it locally using their own data, and then send the computed updates back to the *master* device. The *master* aggregates these updates to improve the global model iteratively.\n",
    "\n",
    "The parameter update rule in such a distributed system can be expressed as:\n",
    "$$\n",
    "x_{t+1} = x_t - \\eta \\frac{1}{N} \\sum_{i=1}^N \\nabla f_i(x_t)\n",
    "$$\n",
    "*where:*\n",
    "- $ N $ : Number of devices\n",
    "- $ f_i(x_t) $ : Loss function for data on device $ i $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc484a5",
   "metadata": {},
   "source": [
    "### Project Prerequisites\n",
    "To build a strong foundation for the concepts covered in this project, I highly recommend exploring the playlist [Optimization Algorithms](https://www.youtube.com/watch?v=ee-HYD6kKqM&list=PLXsmhnDvpjORzPelSDs0LSDrfJcqyLlZc) by Professor Constantine Caramanis. This playlist offers an excellent introduction to the foundational ideas implemented in this project and includes proofs of convergence, which help explain why specific algorithms work and why they exist.\n",
    "\n",
    "Additionally, the playlist provides an overview of *Linear Algebra* and the necessary concepts required to develop an intuitive understanding of the optimization methods discussed in this project.\n",
    "\n",
    "The following resources have also been used to understand algorithms discussed in this project: \n",
    "- [A Unified Theory of SGD: Variance Reduction, Sampling, Quantization and Coordinate Descent](https://arxiv.org/abs/1905.11261)\n",
    "- [Proximal Algorithms](https://web.stanford.edu/~boyd/papers/pdf/prox_algs.pdf)\n",
    "- [Proximal SGD with Linear Search](https://github.com/lowks/gdprox/tree/master)\n",
    "- [Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)\n",
    "- [Proximal Gradient Descent](https://www.youtube.com/watch?v=LuLwG1WwDc4)\n",
    "\n",
    "For the implementation, I utilized the *MNIST* dataset, a standard dataset widely used in machine learning for image classification tasks. The algorithms were implemented in PyTorch, where I wrote custom optimizers and used them to train the model. To run these algorithms efficiently, I made use of a cloud-based RTX 3060 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8061bb55-ca85-400c-afde-08040b80ca3f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets\n",
    "import fastai \n",
    "from torchvision.transforms import ToTensor\n",
    "# from fastai.data.core import DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "# from fastai.data.core import DataLoaders\n",
    "# from fastai.callback.core import Callback\n",
    "# from fastai.vision.all import Learner, Metric\n",
    "# from fastai import optimizer\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3efb759c-b439-415a-b852-49ba6288a831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = nn.Sequential(\n",
    "#     nn.Linear(28*28, 30),\n",
    "#     nn.ReLU(), \n",
    "#     nn.Linear(28*28, 10)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "660669d2-0745-4ebc-8fa2-a79d5137601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf5f5446-9293-4d23-8eac-88821434bcbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([256, 1, 28, 28])\n",
      "Shape of y: torch.Size([256]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "378c3815-1e63-49f2-94d0-deda07564892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "141fef0a-e733-4a0e-ac8f-f9423d8c1c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits \n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c40a001-83ae-42f5-844b-d3903322d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrossEntropyLoss for Classification tasks\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75cbfd4",
   "metadata": {},
   "source": [
    "### Basic Stochastic Gradient Descent\n",
    "\n",
    "Every custom optimization class requires a constructor that initializes the class with the model parameters and the hyperparameters necessary for the optimization process. In this implementation:\n",
    "- *self.params* represents the model parameters that are updated in each iteration.\n",
    "- *self.hypers* stores the hyperparameters, such as the learning rate ( $\\eta$ ), used during optimization.\n",
    "\n",
    "A step function must also be implemented. In the context of Stochastic Gradient Descent (SGD), this function updates the model parameters using the following rule:\n",
    "\n",
    "$$\n",
    "x_{t+1} = x_t - \\eta \\nabla f(x_t),\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $x_t$  is the parameter at iteration  $t$ ,\n",
    "- $\\eta$  is the learning rate,\n",
    "- $\\nabla f(x_t)$  is the gradient of the loss function with respect to the parameter  $x_t$ .\n",
    "\n",
    "In the case of SGD, this update is performed on a randomly selected subset (batch) of data, rather than the entire dataset, to reduce computational cost.\n",
    "\n",
    "The zero_grad function is implemented to reset the gradients of the parameters to None before calculating new gradients. This step is essential because PyTorch accumulates gradients by default during backpropagation. Resetting gradients ensures that each optimization step is based on the current computation only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65cebe2e-8647-4ea5-8f96-4e8f76e14854",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDBasic:\n",
    "    def __init__(self, params, lr): \n",
    "        self.params,self.lr = list(params),lr\n",
    "        self.state = {p: {} for p in self.params}\n",
    "        self.hypers = [{'lr': lr}]\n",
    "        \n",
    "    def step(self, *args, **kwargs):\n",
    "        for p in self.params: \n",
    "            p.data -= p.grad.data * self.lr\n",
    "\n",
    "    def zero_grad(self, *args, **kwargs):\n",
    "        for p in self.params:\n",
    "            p.grad = None\n",
    "\n",
    "    def set_hypers(self, **kwargs):\n",
    "        if 'lr' in kwargs:\n",
    "            self.lr = kwargs['lr']\n",
    "            self.hypers[0]['lr'] = kwargs['lr']\n",
    "            \n",
    "# optimizer = BasicOptimizer(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146ae2ae",
   "metadata": {},
   "source": [
    "### Proximal Gradient Descent\n",
    "\n",
    "Proximal Gradient Descent extends the basic gradient descent algorithm by incorporating a regularization term  R  using its proximal operator. Instead of directly calculating and updating the parameters, the algorithm applies the proximal operator of  R  to the new iterate. The update step is expressed as:\n",
    "\n",
    "$$\n",
    "x_{t+1} = \\text{prox}_{\\gamma R}(x_t - \\gamma g_t),\n",
    "$$\n",
    "\n",
    "where:\n",
    "- g_t  is an unbiased estimator of the gradient (i.e., a stochastic gradient),\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[g_t | x_t] = \\nabla f(x_t),\n",
    "$$\n",
    "\n",
    "ensuring that  $g_t$  approximates the true gradient  $\\nabla f(x_t)$  in expectation.\n",
    "- The proximal operator  $\\text{prox}_{\\gamma R}(x)$  is defined as:\n",
    "\n",
    "$$\n",
    "\\text{prox}_{\\gamma R}(x) := \\arg\\min_u \\left\\{\\gamma R(u) + \\frac{1}{2} |u - x|^2 \\right\\}.\n",
    "$$\n",
    "\n",
    "This method is particularly useful when  R  is non-differentiable (e.g.,  $L1-regularization$), as it allows for the efficient handling of such terms while ensuring convergence properties.\n",
    "\n",
    "In this project, I make use of the  L1  regularizer, which promotes sparsity in the model weights. The proximal operator for  L1  regularization simplifies to the soft-thresholding operator (refer [Proximal algorithms](https://web.stanford.edu/~boyd/papers/pdf/prox_algs.pdf) page 177), defined as:\n",
    "\n",
    "$$\n",
    "\\text{soft_threshold}(x, \\lambda) =\n",
    "\\begin{cases}\n",
    "x - \\lambda, & x > \\lambda \\\\\n",
    "0, & |x| \\leq \\lambda \\\\\n",
    "x + \\lambda, & x < -\\lambda \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This encourages small parameter values to shrink to zero, effectively reducing the model complexity and which could be beneficial in reducing bandwidth while communicating model weights.\n",
    "\n",
    "For this variant of *Proximal SGD*, I make use of a constant learning rate ( $\\gamma$ ). While this approach simplifies the implementation and training process, it cannot guarantee a reduction in the objective function value across iterations, as convergence often requires an adaptive or decaying learning rate. Despite this limitation, the method is effective for exploring the impact of proximal operators on optimization with sparsity constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5752d5e-bc9b-4517-91e2-a0c93fae306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr corresponds to gamma\n",
    "class ProxSGD:\n",
    "    def __init__(self, params, lr):\n",
    "        self.params, self.lr = list(params), lr\n",
    "        self.state = {p: {} for p in self.params}\n",
    "        self.hypers = [{'lr': lr}]\n",
    "    \n",
    "    def soft_threshold(self, x, eta):\n",
    "        # Apply the soft-thresholding operator\n",
    "        return F.softshrink(x, lambd=eta)\n",
    "        \n",
    "    def prox_operator(self, x, eta):\n",
    "        # Use the soft-thresholding operator as the proximal step\n",
    "        return self.soft_threshold(x, eta)\n",
    "\n",
    "    def Gt(self, x, eta, x_grad):\n",
    "        return (1/self.lr) * (x - self.prox_operator(x - self.lr * x_grad, eta))\n",
    "        \n",
    "    def step(self, *args, **kwargs):\n",
    "        for p in self.params:\n",
    "            if p.grad is not None:  # Ensure gradients exist\n",
    "                p.data -= self.lr * self.Gt(p.data, 1e-5, p.grad.data)\n",
    "\n",
    "    def zero_grad(self, *args, **kwargs):\n",
    "        for p in self.params:\n",
    "            p.grad = None\n",
    "\n",
    "    def set_hypers(self, **kwargs):\n",
    "        if 'lr' in kwargs:\n",
    "            self.lr = kwargs['lr']\n",
    "            self.hypers[0]['lr'] = kwargs['lr']\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b0bb94",
   "metadata": {},
   "source": [
    "### Proximal Gradient Descent with Line Search\n",
    "\n",
    "In the standard Proximal Gradient Descent algorithm (as implemented previously), the learning rate ( $\\gamma$ ) is constant. While this simplifies the algorithm, it can lead to suboptimal performance when the constant step size fails to guarantee a reduction in the objective function. This limitation is particularly problematic when the objective function is non-smooth or exhibits steep gradients.\n",
    "\n",
    "The Proximal Gradient Descent with Line Search algorithm addresses this issue by dynamically adjusting the step size ( $\\gamma$ ) during training. This ensures that each update reduces the loss function. \n",
    "\n",
    "Refer the following resources to get a more clear understanding for this algorithm:\n",
    "- [Proximal Algorithms](https://web.stanford.edu/~boyd/papers/pdf/prox_algs.pdf) Page 148\n",
    "- [Proximal SGD with Linear Search](https://github.com/lowks/gdprox/tree/master) : An alternative implementation of the algorithm\n",
    "- [Proximal Gradient Descent](https://www.youtube.com/watch?v=LuLwG1WwDc4) : Youtube video for the explanation of the algorithm\n",
    "\n",
    "There are different ways to determine a step size ( $\\gamma$ ) that reduces the cost function. In this implementation, I use a basic approach by stopping the algorithm at a value where I get reduced cost, but a more sophisticated version is described by Beck and Teboulle in their work on Proximal Algorithms. The method ensures that the step size is adaptively adjusted to guarantee convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d26a792-d3e2-418b-ba34-4139cdf37538",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProxSGDWithLinearSearch:\n",
    "    def __init__(self, params, lr):\n",
    "        self.params, self.lr = list(params), lr\n",
    "        self.state = {p: {} for p in self.params}\n",
    "        self.hypers = [{'lr': lr}]\n",
    "        self.max_iter = 5\n",
    "        self.eta = 1e-5\n",
    "        \n",
    "    def soft_threshold(self, x, eta):\n",
    "        # Apply the soft-thresholding operator\n",
    "        return F.softshrink(x, lambd=eta)\n",
    "        \n",
    "    def prox_operator(self, x):\n",
    "        # Use the soft-thresholding operator as the proximal step\n",
    "        return self.soft_threshold(x, self.eta)\n",
    "\n",
    "    def Gt(self, x, step_size, x_grad):\n",
    "        return (1/step_size) * (x - self.prox_operator(x - step_size * x_grad))\n",
    "        \n",
    "    def step(self, *args, **kwargs):\n",
    "        model = kwargs.get(\"model\")\n",
    "        loss_fn = kwargs.get(\"loss_fn\")\n",
    "        X = kwargs.get(\"X\")\n",
    "        y = kwargs.get(\"y\")\n",
    "        \n",
    "        orig_params = [p.data.clone() for p in self.params]\n",
    "        step_size = self.lr\n",
    "        with torch.no_grad():\n",
    "            pred = model(X)\n",
    "            old_loss = loss_fn(pred, y)\n",
    "        \n",
    "        for _ in range(self.max_iter):\n",
    "            for p in self.params:\n",
    "                if p.grad is not None: \n",
    "                    Gt_val = self.Gt(p.data, step_size, p.grad.data)\n",
    "                    p.data = p.data - step_size * Gt_val\n",
    "            with torch.no_grad():\n",
    "                pred = model(X)\n",
    "                new_loss = loss_fn(pred, y)\n",
    "            if new_loss < old_loss:\n",
    "                break\n",
    "            else:\n",
    "                for i, j in zip(self.params, orig_params):\n",
    "                    i.data.copy_(j)\n",
    "                step_size *= 0.5\n",
    "        else: \n",
    "            for p in self.params:\n",
    "                if p.grad is not None: \n",
    "                    Gt_val = self.Gt(p.data, step_size, p.grad.data)\n",
    "                    p.data = p.data - step_size * Gt_val\n",
    "        self.lr = step_size\n",
    "    def zero_grad(self, *args, **kwargs):\n",
    "        for p in self.params:\n",
    "            p.grad = None\n",
    "\n",
    "    def set_hypers(self, **kwargs):\n",
    "        if 'lr' in kwargs:\n",
    "            self.lr = kwargs['lr']\n",
    "            self.hypers[0]['lr'] = kwargs['lr']\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80aa3016-88df-4167-9148-7817d7678c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step(model=model, loss_fn=loss_fn, X=X, y=y)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f\"BATCH: {batch} of {size/batch_size} batches\")\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dc6150d-230f-4ce7-a3b7-74a623483185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f26d4980-0148-4ca4-b4d8-e71c832e8d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH: 0 of 234.375 batches\n",
      "loss: 2.291508  [  256/60000]\n",
      "BATCH: 100 of 234.375 batches\n",
      "loss: 0.352378  [25856/60000]\n",
      "BATCH: 200 of 234.375 batches\n",
      "loss: 0.315699  [51456/60000]\n",
      "BATCH: 0 of 234.375 batches\n",
      "loss: 0.289511  [  256/60000]\n",
      "BATCH: 100 of 234.375 batches\n",
      "loss: 0.245460  [25856/60000]\n",
      "BATCH: 200 of 234.375 batches\n",
      "loss: 0.208257  [51456/60000]\n",
      "BATCH: 0 of 234.375 batches\n",
      "loss: 0.204918  [  256/60000]\n",
      "BATCH: 100 of 234.375 batches\n",
      "loss: 0.208769  [25856/60000]\n",
      "BATCH: 200 of 234.375 batches\n",
      "loss: 0.162873  [51456/60000]\n",
      "BATCH: 0 of 234.375 batches\n",
      "loss: 0.168498  [  256/60000]\n",
      "BATCH: 100 of 234.375 batches\n",
      "loss: 0.185506  [25856/60000]\n",
      "BATCH: 200 of 234.375 batches\n",
      "loss: 0.139565  [51456/60000]\n",
      "BATCH: 0 of 234.375 batches\n",
      "loss: 0.143701  [  256/60000]\n",
      "BATCH: 100 of 234.375 batches\n",
      "loss: 0.167683  [25856/60000]\n",
      "BATCH: 200 of 234.375 batches\n",
      "loss: 0.125072  [51456/60000]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    train(train_dataloader, model, loss_fn, ProxSGDWithLinearSearch(model.parameters(), 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dde8d310-22a8-4136-a392-e6b973f2210d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 95.4%, Avg loss: 0.149815 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2dfa6b08-09a2-4b1d-a495-17c7dff6ff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some junk code\n",
    "\n",
    "# dls = DataLoaders(train_dataloader, test_dataloader)\n",
    "\n",
    "# class CustomAccuracy1(Metric):\n",
    "#     def __init__(self):\n",
    "#         self.correct = 0\n",
    "#         self.total = 0\n",
    "\n",
    "#     def reset(self):\n",
    "#         self.correct = 0\n",
    "#         self.total = 0\n",
    "\n",
    "#     def accumulate(self, learn):\n",
    "#         # Get max probability of the final dimension\n",
    "#         preds = learn.pred.argmax(dim=-1)\n",
    "#         self.correct += (preds == learn.y).sum().item()\n",
    "#         self.total += len(learn.y)\n",
    "\n",
    "#     @property\n",
    "#     def value(self):\n",
    "#         return (self.correct / self.total) * 100 if self.total > 0 else None\n",
    "\n",
    "#     @property\n",
    "#     def name(self):\n",
    "#         return \"Accuracy\"\n",
    "\n",
    "# class CustomAccuracy2(Metric):\n",
    "#     def __init__(self):\n",
    "#         self.correct = 0\n",
    "#         self.total = 0\n",
    "\n",
    "#     def reset(self):\n",
    "#         self.correct = 0\n",
    "#         self.total = 0\n",
    "\n",
    "#     def accumulate(self, learn):\n",
    "#         # Get max probability of the final dimension\n",
    "#         preds = learn.pred.argmax(dim=-1)\n",
    "#         self.correct += (preds == learn.y).sum().item()\n",
    "#         self.total += len(learn.y)\n",
    "\n",
    "#     @property\n",
    "#     def value(self):\n",
    "#         return (self.correct / self.total) * 100 if self.total > 0 else None\n",
    "\n",
    "#     @property\n",
    "#     def name(self):\n",
    "#         return \"meowMeow2\"\n",
    "\n",
    "# class MixAccuracy(Metric):\n",
    "#     def __init__(self):\n",
    "#         self.c1 = CustomAccuracy1()\n",
    "#         self.c2 = CustomAccuracy2()\n",
    "\n",
    "#     def reset(self):\n",
    "#         self.c1.reset()\n",
    "#         self.c2.reset()\n",
    "\n",
    "#     def accumulate(self, learn):\n",
    "#         self.c1.accumulate(learn)\n",
    "#         self.c2.accumulate(learn)\n",
    "\n",
    "#     @property\n",
    "#     def value(self):\n",
    "#         return self.c1.value, self.c2.value\n",
    "\n",
    "#     @property\n",
    "#     def name(self):\n",
    "#         return f\"{self.c1.name}_{self.c2.name}\"\n",
    "\n",
    "# class RegularizationCallback(Callback):\n",
    "#     # def __init__(self, model, lambda_reg):\n",
    "#     #     self.model = model\n",
    "#     #     self.lambda_reg = lambda_reg\n",
    "#     def __init__(self):\n",
    "#         self.lambda_reg = 1e-3\n",
    "#     def after_loss(self):\n",
    "#         # Compute L2 regularization term\n",
    "#         l1_reg = 0\n",
    "#         for param in self.model.parameters():\n",
    "#             l1_reg += torch.sum(torch.abs(param))\n",
    "#         # self.learn.loss += self.lambda_reg * l1_reg * self.lr\n",
    "#     def after_epoch(self):\n",
    "#         print(self.loss)\n",
    "        \n",
    "        \n",
    "\n",
    "# learner = Learner(dls, model, opt_func=ProxSGD, loss_func=loss_fn, metrics=CustomAccuracy1)\n",
    "\n",
    "\n",
    "\n",
    "# df = learner.fit(20, lr=1)\n",
    "# df\n",
    "\n",
    "# s = 0\n",
    "# for param in model.parameters():\n",
    "#     s += torch.sum(torch.abs(param.data) <= .01) \n",
    "# print(s)\n",
    "\n",
    "# s = 0\n",
    "# for param in model.parameters():\n",
    "#     s += torch.sum(torch.abs(param))\n",
    "# print(s)\n",
    "\n",
    "# m = nn.Threshold(0.1, 20)\n",
    "# input = torch.randn(2)\n",
    "# output = m(input)\n",
    "\n",
    "# output, input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6edf5b-9e56-41a3-93a0-4092fa30a3ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
